{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempt, treat all stocks and all dates independently and predict last two values with only 44 features per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "% matplotlib inline  \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import logging\n",
    "import imp\n",
    "imp.reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class StockData():\n",
    "    def __init__(self, batch_size=64, train_date_range=('2018-01-16', '2018-01-16'), \n",
    "                 test_date_range=('2018-01-17', '2018-01-19')):\n",
    "        self.name = 'independent'\n",
    "        self.stock_dir = './stocks_by_date'\n",
    "        self.train_date_range = train_date_range\n",
    "        self.test_date_range = test_date_range\n",
    "        self.use_dates = []\n",
    "        self.train_stocks = self.get_stocks('train')\n",
    "        self.test_stocks = self.get_stocks('test')\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.train_idx = np.random.permutation(len(self.train_stocks))\n",
    "        self.train_cnt = 0\n",
    "        self.test_idx = np.random.permutation(len(self.test_stocks))\n",
    "        self.test_cnt = 0\n",
    "        \n",
    "        print('number of training stocks: %d, number of testing stocks: %d' % \n",
    "              (self.train_stocks.shape[0], self.test_stocks.shape[0]))\n",
    "        print('use below dates:', self.use_dates)\n",
    "                \n",
    "    def get_stocks(self, status='train'):\n",
    "        if status == 'train':\n",
    "            start_date = time.strptime(self.train_date_range[0], '%Y-%m-%d')\n",
    "            end_date = time.strptime(self.train_date_range[1], '%Y-%m-%d')\n",
    "        else:\n",
    "            start_date = time.strptime(self.test_date_range[0], '%Y-%m-%d')\n",
    "            end_date = time.strptime(self.test_date_range[1], '%Y-%m-%d')\n",
    "        stocks = []\n",
    "        stock_files = os.listdir(self.stock_dir)\n",
    "        stock_files.sort()\n",
    "        for stock_file in stock_files:\n",
    "            orig_date, _ = os.path.splitext(stock_file)\n",
    "            date = time.strptime(orig_date, '%Y-%m-%d')\n",
    "            if date < start_date or date > end_date:\n",
    "                continue\n",
    "            self.use_dates.append(orig_date)\n",
    "            stock_file_name = os.path.join(self.stock_dir, stock_file)\n",
    "            lines = open(stock_file_name).readlines()\n",
    "            for line in lines:\n",
    "                line = line.strip().split(', ')\n",
    "                one_id = line[0]\n",
    "                name = line[1]\n",
    "                feature = line[2:]\n",
    "                stocks.append(feature)\n",
    "        stocks = np.array(stocks).astype(np.float32)\n",
    "        return stocks\n",
    "\n",
    "    def get_next_batch(self, status='train'):\n",
    "        inputs = np.zeros((self.batch_size, 44))\n",
    "        outputs = np.zeros((self.batch_size, 1))\n",
    "        i = 0\n",
    "        restart = False\n",
    "        while i < self.batch_size:\n",
    "            if status == 'train':\n",
    "                x = self.train_stocks[self.train_idx[self.train_cnt], 0:44]\n",
    "                y = self.train_stocks[self.train_idx[self.train_cnt], 44]\n",
    "                self.train_cnt = self.train_cnt + 1\n",
    "                if self.train_cnt >= self.train_stocks.shape[0]:\n",
    "                    self.train_idx = np.random.permutation(self.train_stocks.shape[0])\n",
    "                    self.train_cnt = 0\n",
    "                    restart = True\n",
    "            else:\n",
    "                x = self.test_stocks[self.test_idx[self.test_cnt], 0:44]\n",
    "                y = self.test_stocks[self.test_idx[self.test_cnt], 44]\n",
    "                self.test_cnt = self.test_cnt + 1\n",
    "                if self.test_cnt >= self.test_stocks.shape[0]:\n",
    "                    self.test_idx = np.random.permutation(self.test_stocks.shape[0])\n",
    "                    self.test_cnt = 0\n",
    "                    restart = True\n",
    "            if np.isnan(x).any() or np.isnan(y).any():\n",
    "                continue\n",
    "            inputs[i, :] = x * 100\n",
    "            outputs[i, :] = y * 100\n",
    "            i = i + 1\n",
    "        return inputs, outputs, restart\n",
    "    \n",
    "    def get_one_sample(self, stock_name='', date=''):\n",
    "        inputs = np.zeros((1, 44))\n",
    "        outputs = np.zeros((1, 2))\n",
    "        if stock_name == '':\n",
    "            stock_name = '000001.XSHE'\n",
    "        if date == '':\n",
    "            date = '2018-01-17'\n",
    "        stock_file = os.path.join('stocks_by_name', stock_name + '.txt')\n",
    "        lines = open(stock_file).readlines()\n",
    "        for line in lines:\n",
    "            if date in line:\n",
    "                break\n",
    "        line = line.strip().split(', ')\n",
    "        feature = np.array(line[2:]).astype(np.float32)\n",
    "        inputs[0, :] = feature[:44] * 100\n",
    "        outputs[0, :] = feature[44] * 100\n",
    "        return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training stocks: 3461, number of testing stocks: 10382\n",
      "('use below dates:', ['2018-01-16', '2018-01-17', '2018-01-18', '2018-01-19'])\n",
      "(array([[-2.08699989e+00, -4.84500027e+00, -3.01900005e+00, ...,\n",
      "        -1.36104004e+02,  2.52946094e+03,  2.45077002e+03],\n",
      "       [-1.56000003e-01, -3.96399975e+00,  1.50000006e-01, ...,\n",
      "        -6.97579956e+01,  6.43867004e+02,  6.41538025e+02],\n",
      "       [ 7.82999933e-01, -5.58799982e+00, -8.10999990e-01, ...,\n",
      "         6.97300034e+01,  5.35139062e+03,  5.37800879e+03],\n",
      "       ...,\n",
      "       [ 0.00000000e+00, -1.56099999e+00, -1.25999999e+00, ...,\n",
      "        -4.65429993e+01,  6.97872009e+02,  6.97442017e+02],\n",
      "       [ 2.16999993e-01, -5.19999981e-01, -5.60999990e-01, ...,\n",
      "         5.28499985e+00,  2.30198901e+03,  2.30557495e+03],\n",
      "       [ 1.43599999e+00, -5.28299999e+00,  1.03399992e+00, ...,\n",
      "         1.37792999e+02,  5.54234009e+02,  5.60276978e+02]]), array([[-0.68999999],\n",
      "       [-0.92099998],\n",
      "       [ 2.15499997],\n",
      "       [ 1.60099994],\n",
      "       [-2.40400005],\n",
      "       [ 4.37900014],\n",
      "       [-3.78599986],\n",
      "       [-4.83799987],\n",
      "       [ 3.88099998],\n",
      "       [ 0.14900001],\n",
      "       [ 2.95100007],\n",
      "       [ 1.01800002],\n",
      "       [-1.00699998],\n",
      "       [ 4.45200019],\n",
      "       [-0.81599997],\n",
      "       [ 3.06000002],\n",
      "       [ 0.163     ],\n",
      "       [ 0.45799999],\n",
      "       [-0.22799999],\n",
      "       [ 3.39200012],\n",
      "       [ 0.655     ],\n",
      "       [-0.97700004],\n",
      "       [ 0.72599999],\n",
      "       [-0.91399997],\n",
      "       [ 2.87599992],\n",
      "       [-2.92799994],\n",
      "       [ 0.35999999],\n",
      "       [ 0.60000001],\n",
      "       [ 0.36599999],\n",
      "       [-2.25000009],\n",
      "       [-0.48799999],\n",
      "       [ 2.568     ],\n",
      "       [ 0.593     ],\n",
      "       [ 0.33199999],\n",
      "       [ 5.66200018],\n",
      "       [-4.11299989],\n",
      "       [ 0.045     ],\n",
      "       [ 0.42500002],\n",
      "       [ 0.58800001],\n",
      "       [-0.184     ],\n",
      "       [ 1.64299998],\n",
      "       [ 7.0419997 ],\n",
      "       [ 1.00999996],\n",
      "       [ 0.093     ],\n",
      "       [-1.15499999],\n",
      "       [-0.74999998],\n",
      "       [ 1.602     ],\n",
      "       [-1.31299999],\n",
      "       [-0.47399998],\n",
      "       [-0.72599999],\n",
      "       [-0.202     ],\n",
      "       [ 0.85399998],\n",
      "       [ 0.41899998],\n",
      "       [ 0.05      ],\n",
      "       [ 0.19      ],\n",
      "       [-0.30400001],\n",
      "       [-5.58300018],\n",
      "       [-0.103     ],\n",
      "       [ 0.70500001],\n",
      "       [ 1.67699996],\n",
      "       [-1.28199998],\n",
      "       [-1.17899999],\n",
      "       [ 0.97899996],\n",
      "       [-1.84799992]]))\n",
      "(array([[ 5.59000015e-01,  0.00000000e+00, -3.03499985e+00, ...,\n",
      "         5.33200026e+00,  3.53675018e+02,  3.58162994e+02],\n",
      "       [-4.49999981e-02, -4.88999993e-01, -1.89100003e+00, ...,\n",
      "        -2.40360012e+01,  2.25166406e+03,  2.23904590e+03],\n",
      "       [-4.25999999e-01,  0.00000000e+00, -1.26900005e+00, ...,\n",
      "        -4.75139999e+01,  2.33143997e+02,  2.34259003e+02],\n",
      "       ...,\n",
      "       [-3.72999996e-01, -2.39199996e+00,  9.12999988e-01, ...,\n",
      "        -6.50289993e+01,  5.39240967e+02,  5.35036011e+02],\n",
      "       [ 2.31500006e+00,  1.65899992e+00, -2.28999996e+00, ...,\n",
      "         1.28366013e+02,  1.58012402e+03,  1.60979102e+03],\n",
      "       [-2.69900012e+00,  2.59299994e+00,  2.17300010e+00, ...,\n",
      "        -1.68820999e+02,  3.86045801e+03,  3.76416699e+03]]), array([[-8.87000002e-01],\n",
      "       [ 4.60599996e+00],\n",
      "       [-1.09999999e-01],\n",
      "       [ 1.39400000e+00],\n",
      "       [-2.40999996e-01],\n",
      "       [ 2.19999999e+00],\n",
      "       [-3.19999992e-01],\n",
      "       [ 7.38999993e-01],\n",
      "       [ 1.25500001e+00],\n",
      "       [-4.99999989e-01],\n",
      "       [ 1.02800000e+00],\n",
      "       [ 1.17499996e+00],\n",
      "       [ 6.00000005e-01],\n",
      "       [-1.79999997e-01],\n",
      "       [-3.41700017e+00],\n",
      "       [ 3.26999999e-01],\n",
      "       [ 7.09999993e-02],\n",
      "       [ 4.91000013e-01],\n",
      "       [ 3.05000003e+00],\n",
      "       [-6.64999988e-01],\n",
      "       [-1.10000001e-02],\n",
      "       [ 2.27000006e-01],\n",
      "       [ 1.25900004e+00],\n",
      "       [ 6.73900023e+00],\n",
      "       [-2.27999990e-01],\n",
      "       [-1.21999998e-01],\n",
      "       [-6.00000028e-02],\n",
      "       [ 1.48000000e+00],\n",
      "       [ 1.91600006e+00],\n",
      "       [-3.28999991e-01],\n",
      "       [-8.80000007e-02],\n",
      "       [ 5.26000001e-01],\n",
      "       [-2.87900008e+00],\n",
      "       [-2.63000000e-01],\n",
      "       [-1.26000005e-01],\n",
      "       [ 1.07000000e-01],\n",
      "       [-1.18199997e+00],\n",
      "       [ 4.67000017e-01],\n",
      "       [ 1.11499997e+00],\n",
      "       [ 1.18199997e+00],\n",
      "       [-8.96999985e-01],\n",
      "       [-2.04200000e+00],\n",
      "       [-9.85000003e-01],\n",
      "       [ 1.13599999e+00],\n",
      "       [-8.46999977e-01],\n",
      "       [ 1.67299993e+00],\n",
      "       [-3.47999996e-01],\n",
      "       [ 9.99999975e-04],\n",
      "       [ 8.20999965e-01],\n",
      "       [ 1.59000009e+00],\n",
      "       [-1.10799996e+00],\n",
      "       [ 3.03300004e+00],\n",
      "       [ 1.37200002e+00],\n",
      "       [-6.58999989e-01],\n",
      "       [ 2.79999990e-01],\n",
      "       [-2.67900005e+00],\n",
      "       [ 3.10000003e-02],\n",
      "       [-1.38100004e+00],\n",
      "       [-6.69000018e-01],\n",
      "       [ 2.82700006e+00],\n",
      "       [-1.27600003e+00],\n",
      "       [ 8.60000029e-01],\n",
      "       [-5.24999993e-01],\n",
      "       [-3.48600000e+00]]))\n",
      "(array([[ 2.10999995e-01,  0.00000000e+00,  4.68599987e+00,\n",
      "         1.09799993e+00,  3.44999999e-01,  2.84899998e+00,\n",
      "        -5.17779999e+01,  2.15252991e+02,  1.40778000e+02,\n",
      "         1.06642998e+02,  1.57309990e+01, -1.33373001e+02,\n",
      "         1.05778000e+02,  4.26010017e+01,  2.62820007e+02,\n",
      "         2.70951996e+02,  2.65558014e+02,  1.01099997e+01,\n",
      "        -2.29499989e+01,  3.30599976e+01, -1.09960999e+02,\n",
      "         1.73408997e+02,  1.79334991e+02,  1.86733002e+02,\n",
      "         9.20449982e+01,  5.70839996e+01,  8.49980011e+01,\n",
      "         6.34430008e+01,  3.10023096e+03,  7.04240036e+01,\n",
      "         7.97070007e+01,  8.71179962e+01,  1.33977698e+03,\n",
      "         2.31599998e+00,  1.84099996e+00,  1.68500006e+00,\n",
      "         1.43299997e+00,  5.80000000e+03,  5.50000000e+03,\n",
      "         4.00000000e+03, -1.34076004e+02,  1.50970001e+01,\n",
      "         1.45919800e+03,  1.42501807e+03]]), array([[2.65900008, 2.65900008]]))\n"
     ]
    }
   ],
   "source": [
    "data = StockData()\n",
    "x, y, _ = data.get_next_batch('train')\n",
    "print(x, y)\n",
    "x, y, _ = data.get_next_batch('test')\n",
    "print(x, y)\n",
    "x, y = data.get_one_sample()\n",
    "print (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Interface(object):\n",
    "    def __init__(self, data, model, learning_rate, train_iter, test_iter, test_interval, save_interval, \n",
    "                 init_model_path, save_model_path, tensorboard_path):\n",
    "        self.data = data\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_iter = train_iter\n",
    "        self.test_iter = test_iter\n",
    "        self.test_interval = test_interval\n",
    "        self.save_interval = save_interval\n",
    "        self.init_model_path = init_model_path\n",
    "        self.save_model_path = save_model_path\n",
    "        self.tensorboard_path = tensorboard_path\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.init_model()\n",
    "    \n",
    "    def init_model(self):\n",
    "        # model = torch.nn.DataParallel(model).cuda()\n",
    "        self.model = self.model.to(self.device)\n",
    "        if self.init_model_path is not '':\n",
    "            self.model.load_state_dict(torch.load(self.init_model_path))\n",
    "        return self.model\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        writer = SummaryWriter(self.tensorboard_path)\n",
    "        criterion = nn.L1Loss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        train_loss_all = []\n",
    "        for it in range(self.train_iter):\n",
    "            x, y, _ = self.data.get_next_batch('train')\n",
    "            x = torch.from_numpy(x).float().to(self.device)\n",
    "            y = torch.from_numpy(y).float().to(self.device)\n",
    "            optimizer.zero_grad()\n",
    "            pred = self.model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            writer.add_scalar('train_loss', loss, it)\n",
    "            train_loss_all.append(loss)\n",
    "            if len(train_loss_all) > 100:\n",
    "                train_loss_all.pop(0)\n",
    "            ave_train_loss = sum(train_loss_all) / float(len(train_loss_all))\n",
    "            if (it + 1) % 10 == 0:\n",
    "                logging.info('iteration %d, train loss: %.3f, average train loss: %.3f', it, loss, ave_train_loss)\n",
    "            if (it + 1) % self.save_interval == 0:\n",
    "                logging.info('iteration %d, saving model', it)\n",
    "                with open(self.save_model_path, 'w') as handle:\n",
    "                    torch.save(self.model.state_dict(), handle)\n",
    "            if (it + 1) % self.test_interval == 0:\n",
    "                logging.info('iteration %d, testing', it)\n",
    "                test_loss = self.test()\n",
    "                writer.add_scalar('test_loss', test_loss, it)\n",
    "                self.model.train()\n",
    "                torch.set_grad_enabled(True)\n",
    "        writer.close()\n",
    "\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "        test_loss_all = []\n",
    "        criterion = nn.L1Loss()\n",
    "        for it in range(self.test_iter):\n",
    "            x, y, _ = self.data.get_next_batch('test')\n",
    "            x = torch.from_numpy(x).float().to(self.device)\n",
    "            y = torch.from_numpy(y).float().to(self.device)\n",
    "            pred = self.model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            test_loss_all.append(loss)\n",
    "            if len(test_loss_all) > 100:\n",
    "                test_loss_all.pop(0)\n",
    "        test_loss = np.mean(np.array(test_loss_all))\n",
    "        logging.info('average test loss: %.3f', test_loss)\n",
    "        return test_loss\n",
    "    \n",
    "    def test_all(self):\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "        test_loss_all = []\n",
    "        criterion = nn.L1Loss()\n",
    "        self.data.test_cnt = 0 # Restart from the first testing batch, note that this is important if you test() before.\n",
    "        while True:\n",
    "            x, y, restart = self.data.get_next_batch('test')   \n",
    "            if restart:\n",
    "                break\n",
    "            x = torch.from_numpy(x).float().to(self.device)\n",
    "            y = torch.from_numpy(y).float().to(self.device)\n",
    "            pred = self.model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            test_loss_all.append(loss)\n",
    "        test_loss = np.mean(np.array(test_loss_all))\n",
    "        logging.info('overall average test loss: %.3f', test_loss)\n",
    "    \n",
    "    def baseline(self):\n",
    "        torch.set_grad_enabled(False)\n",
    "        test_loss_all = []\n",
    "        criterion = nn.L1Loss()\n",
    "        self.data.test_cnt = 0 # Restart from the first testing batch, note that this is important if you test() before.\n",
    "        while True:\n",
    "            x, y, restart = self.data.get_next_batch('test')   \n",
    "            if restart:\n",
    "                break\n",
    "            pred = np.zeros((self.data.batch_size, 1))\n",
    "            pred = torch.from_numpy(pred).float().to(self.device)\n",
    "            y = torch.from_numpy(y).float().to(self.device)\n",
    "            loss = criterion(pred, y)\n",
    "            test_loss_all.append(loss)\n",
    "        test_loss = np.mean(np.array(test_loss_all))\n",
    "        logging.info('baseline average test loss: %.3f', test_loss)\n",
    "    \n",
    "    def predict(self, stock_name='', date=''):\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "        test_loss_all = []\n",
    "        criterion = nn.L1Loss()\n",
    "        x, y = self.data.get_one_sample(stock_name, date)\n",
    "        x = torch.from_numpy(x).float().to(self.device)\n",
    "        y = torch.from_numpy(y).float().to(self.device)\n",
    "        pred = np.zeros((1, 1))\n",
    "        pred = torch.from_numpy(pred).float().to(self.device)\n",
    "        loss = criterion(pred, y)\n",
    "        logging.info('baseline loss: %.3f', loss)\n",
    "        pred = self.model(x)\n",
    "        loss = criterion(pred, y)\n",
    "        logging.info('prediction loss: %.3f', loss)\n",
    "        return x, pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BaseNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(BaseNet, self).__init__()\n",
    "        num_hidden = 256\n",
    "        self.bn0 = nn.BatchNorm1d(input_size)\n",
    "        self.fc1 = nn.Linear(input_size, num_hidden)\n",
    "        self.bn1 = nn.BatchNorm1d(num_hidden)\n",
    "        self.fc2 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.bn2 = nn.BatchNorm1d(num_hidden)\n",
    "        self.fc3 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.bn3 = nn.BatchNorm1d(num_hidden)\n",
    "        self.fc4 = nn.Linear(num_hidden, num_hidden)\n",
    "        self.bn4 = nn.BatchNorm1d(num_hidden)\n",
    "        self.fc = nn.Linear(num_hidden, output_size)\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn0(x)\n",
    "        # x = F.dropout(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        # x = F.dropout(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        # x = F.dropout(x)\n",
    "        x = F.relu(self.bn3(self.fc3(x)))\n",
    "        # x = F.dropout(x)\n",
    "        x = F.relu(self.bn4(self.fc4(x)))\n",
    "        # x = F.dropout(x)\n",
    "        pred = self.fc(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:38:16 INFO:baseline average test loss: 1.250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training stocks: 3461, number of testing stocks: 10382\n",
      "('use below dates:', ['2018-01-16', '2018-01-17', '2018-01-18', '2018-01-19'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:38:16 INFO:iteration 9, train loss: 1.593, average train loss: 1.583\n",
      "10:38:17 INFO:iteration 19, train loss: 1.361, average train loss: 1.526\n",
      "10:38:17 INFO:iteration 29, train loss: 1.607, average train loss: 1.485\n",
      "10:38:17 INFO:iteration 39, train loss: 1.230, average train loss: 1.444\n",
      "10:38:17 INFO:iteration 49, train loss: 1.260, average train loss: 1.421\n",
      "10:38:17 INFO:iteration 59, train loss: 1.191, average train loss: 1.392\n",
      "10:38:17 INFO:iteration 69, train loss: 1.324, average train loss: 1.368\n",
      "10:38:17 INFO:iteration 79, train loss: 1.263, average train loss: 1.348\n",
      "10:38:17 INFO:iteration 89, train loss: 1.200, average train loss: 1.321\n",
      "10:38:18 INFO:iteration 99, train loss: 1.057, average train loss: 1.304\n",
      "10:38:18 INFO:iteration 99, testing\n",
      "10:38:18 INFO:average test loss: 1.519\n",
      "10:38:18 INFO:iteration 109, train loss: 1.137, average train loss: 1.258\n",
      "10:38:18 INFO:iteration 119, train loss: 1.033, average train loss: 1.221\n",
      "10:38:18 INFO:iteration 129, train loss: 1.103, average train loss: 1.193\n",
      "10:38:18 INFO:iteration 139, train loss: 1.041, average train loss: 1.170\n",
      "10:38:18 INFO:iteration 149, train loss: 1.111, average train loss: 1.143\n",
      "10:38:18 INFO:iteration 159, train loss: 1.076, average train loss: 1.120\n",
      "10:38:19 INFO:iteration 169, train loss: 0.916, average train loss: 1.098\n",
      "10:38:19 INFO:iteration 179, train loss: 1.002, average train loss: 1.075\n",
      "10:38:19 INFO:iteration 189, train loss: 0.926, average train loss: 1.060\n",
      "10:38:19 INFO:iteration 199, train loss: 0.900, average train loss: 1.041\n",
      "10:38:19 INFO:iteration 199, testing\n",
      "10:38:19 INFO:average test loss: 1.700\n",
      "10:38:19 INFO:iteration 209, train loss: 0.880, average train loss: 1.023\n",
      "10:38:19 INFO:iteration 219, train loss: 1.119, average train loss: 1.006\n",
      "10:38:19 INFO:iteration 229, train loss: 0.875, average train loss: 0.982\n",
      "10:38:19 INFO:iteration 239, train loss: 0.843, average train loss: 0.959\n",
      "10:38:20 INFO:iteration 249, train loss: 0.889, average train loss: 0.942\n",
      "10:38:20 INFO:iteration 259, train loss: 0.711, average train loss: 0.925\n",
      "10:38:20 INFO:iteration 269, train loss: 0.794, average train loss: 0.909\n",
      "10:38:20 INFO:iteration 279, train loss: 0.808, average train loss: 0.894\n",
      "10:38:20 INFO:iteration 289, train loss: 0.854, average train loss: 0.878\n",
      "10:38:20 INFO:iteration 299, train loss: 0.701, average train loss: 0.868\n",
      "10:38:20 INFO:iteration 299, testing\n",
      "10:38:20 INFO:average test loss: 1.756\n",
      "10:38:20 INFO:iteration 309, train loss: 0.840, average train loss: 0.853\n",
      "10:38:20 INFO:iteration 319, train loss: 0.676, average train loss: 0.837\n",
      "10:38:21 INFO:iteration 329, train loss: 0.835, average train loss: 0.827\n",
      "10:38:21 INFO:iteration 339, train loss: 0.775, average train loss: 0.814\n",
      "10:38:21 INFO:iteration 349, train loss: 0.680, average train loss: 0.800\n",
      "10:38:21 INFO:iteration 359, train loss: 0.717, average train loss: 0.789\n",
      "10:38:21 INFO:iteration 369, train loss: 0.717, average train loss: 0.775\n",
      "10:38:21 INFO:iteration 379, train loss: 0.770, average train loss: 0.763\n",
      "10:38:21 INFO:iteration 389, train loss: 0.613, average train loss: 0.752\n",
      "10:38:21 INFO:iteration 399, train loss: 0.819, average train loss: 0.744\n",
      "10:38:21 INFO:iteration 399, testing\n",
      "10:38:21 INFO:average test loss: 1.713\n",
      "10:38:22 INFO:iteration 409, train loss: 0.704, average train loss: 0.737\n",
      "10:38:22 INFO:iteration 419, train loss: 0.657, average train loss: 0.732\n",
      "10:38:22 INFO:iteration 429, train loss: 0.763, average train loss: 0.723\n",
      "10:38:22 INFO:iteration 439, train loss: 0.680, average train loss: 0.715\n",
      "10:38:22 INFO:iteration 449, train loss: 0.618, average train loss: 0.705\n",
      "10:38:22 INFO:iteration 459, train loss: 0.609, average train loss: 0.697\n",
      "10:38:22 INFO:iteration 469, train loss: 0.714, average train loss: 0.689\n",
      "10:38:22 INFO:iteration 479, train loss: 0.619, average train loss: 0.683\n",
      "10:38:22 INFO:iteration 489, train loss: 0.576, average train loss: 0.676\n",
      "10:38:23 INFO:iteration 499, train loss: 0.651, average train loss: 0.664\n",
      "10:38:23 INFO:iteration 499, testing\n",
      "10:38:23 INFO:average test loss: 1.700\n",
      "10:38:23 INFO:iteration 509, train loss: 0.562, average train loss: 0.651\n",
      "10:38:23 INFO:iteration 519, train loss: 0.577, average train loss: 0.638\n",
      "10:38:23 INFO:iteration 529, train loss: 0.719, average train loss: 0.633\n",
      "10:38:23 INFO:iteration 539, train loss: 0.675, average train loss: 0.628\n",
      "10:38:23 INFO:iteration 549, train loss: 0.638, average train loss: 0.623\n",
      "10:38:23 INFO:iteration 559, train loss: 0.634, average train loss: 0.620\n",
      "10:38:23 INFO:iteration 569, train loss: 0.514, average train loss: 0.613\n",
      "10:38:24 INFO:iteration 579, train loss: 0.529, average train loss: 0.608\n",
      "10:38:24 INFO:iteration 589, train loss: 0.418, average train loss: 0.598\n",
      "10:38:24 INFO:iteration 599, train loss: 0.552, average train loss: 0.590\n",
      "10:38:24 INFO:iteration 599, testing\n",
      "10:38:24 INFO:average test loss: 1.729\n",
      "10:38:24 INFO:iteration 609, train loss: 0.580, average train loss: 0.588\n",
      "10:38:24 INFO:iteration 619, train loss: 0.622, average train loss: 0.586\n",
      "10:38:24 INFO:iteration 629, train loss: 0.537, average train loss: 0.580\n",
      "10:38:24 INFO:iteration 639, train loss: 0.676, average train loss: 0.575\n",
      "10:38:25 INFO:iteration 649, train loss: 0.590, average train loss: 0.572\n",
      "10:38:25 INFO:iteration 659, train loss: 0.542, average train loss: 0.565\n",
      "10:38:25 INFO:iteration 669, train loss: 0.457, average train loss: 0.559\n",
      "10:38:25 INFO:iteration 679, train loss: 0.441, average train loss: 0.552\n",
      "10:38:25 INFO:iteration 689, train loss: 0.524, average train loss: 0.552\n",
      "10:38:25 INFO:iteration 699, train loss: 0.636, average train loss: 0.551\n",
      "10:38:25 INFO:iteration 699, testing\n",
      "10:38:25 INFO:average test loss: 1.663\n",
      "10:38:25 INFO:iteration 709, train loss: 0.612, average train loss: 0.546\n",
      "10:38:25 INFO:iteration 719, train loss: 0.548, average train loss: 0.538\n",
      "10:38:26 INFO:iteration 729, train loss: 0.483, average train loss: 0.528\n",
      "10:38:26 INFO:iteration 739, train loss: 0.508, average train loss: 0.519\n",
      "10:38:26 INFO:iteration 749, train loss: 0.481, average train loss: 0.508\n",
      "10:38:26 INFO:iteration 759, train loss: 0.484, average train loss: 0.499\n",
      "10:38:26 INFO:iteration 769, train loss: 0.425, average train loss: 0.495\n",
      "10:38:26 INFO:iteration 779, train loss: 0.416, average train loss: 0.491\n",
      "10:38:26 INFO:iteration 789, train loss: 0.439, average train loss: 0.485\n",
      "10:38:26 INFO:iteration 799, train loss: 0.455, average train loss: 0.480\n",
      "10:38:26 INFO:iteration 799, testing\n",
      "10:38:26 INFO:average test loss: 1.699\n",
      "10:38:27 INFO:iteration 809, train loss: 0.442, average train loss: 0.473\n",
      "10:38:27 INFO:iteration 819, train loss: 0.423, average train loss: 0.470\n",
      "10:38:27 INFO:iteration 829, train loss: 0.498, average train loss: 0.466\n",
      "10:38:27 INFO:iteration 839, train loss: 0.440, average train loss: 0.465\n",
      "10:38:27 INFO:iteration 849, train loss: 0.416, average train loss: 0.463\n",
      "10:38:27 INFO:iteration 859, train loss: 0.471, average train loss: 0.460\n",
      "10:38:27 INFO:iteration 869, train loss: 0.408, average train loss: 0.458\n",
      "10:38:27 INFO:iteration 879, train loss: 0.400, average train loss: 0.454\n",
      "10:38:28 INFO:iteration 889, train loss: 0.384, average train loss: 0.449\n",
      "10:38:28 INFO:iteration 899, train loss: 0.457, average train loss: 0.442\n",
      "10:38:28 INFO:iteration 899, testing\n",
      "10:38:28 INFO:average test loss: 1.673\n",
      "10:38:28 INFO:iteration 909, train loss: 0.486, average train loss: 0.438\n",
      "10:38:28 INFO:iteration 919, train loss: 0.498, average train loss: 0.439\n",
      "10:38:28 INFO:iteration 929, train loss: 0.469, average train loss: 0.442\n",
      "10:38:28 INFO:iteration 939, train loss: 0.389, average train loss: 0.441\n",
      "10:38:28 INFO:iteration 949, train loss: 0.517, average train loss: 0.441\n",
      "10:38:28 INFO:iteration 959, train loss: 0.418, average train loss: 0.439\n",
      "10:38:29 INFO:iteration 969, train loss: 0.424, average train loss: 0.436\n",
      "10:38:29 INFO:iteration 979, train loss: 0.392, average train loss: 0.435\n",
      "10:38:29 INFO:iteration 989, train loss: 0.443, average train loss: 0.436\n",
      "10:38:29 INFO:iteration 999, train loss: 0.394, average train loss: 0.439\n",
      "10:38:29 INFO:iteration 999, testing\n",
      "10:38:29 INFO:average test loss: 1.671\n",
      "10:38:29 INFO:iteration 1000, saving model\n",
      "10:38:29 INFO:iteration 1009, train loss: 0.445, average train loss: 0.439\n",
      "10:38:29 INFO:iteration 1019, train loss: 0.453, average train loss: 0.437\n",
      "10:38:29 INFO:iteration 1029, train loss: 0.408, average train loss: 0.435\n",
      "10:38:29 INFO:iteration 1039, train loss: 0.384, average train loss: 0.434\n",
      "10:38:30 INFO:iteration 1049, train loss: 0.427, average train loss: 0.438\n",
      "10:38:30 INFO:iteration 1059, train loss: 0.379, average train loss: 0.437\n",
      "10:38:30 INFO:iteration 1069, train loss: 0.346, average train loss: 0.433\n",
      "10:38:30 INFO:iteration 1079, train loss: 0.421, average train loss: 0.431\n",
      "10:38:30 INFO:iteration 1089, train loss: 0.368, average train loss: 0.428\n",
      "10:38:30 INFO:iteration 1099, train loss: 0.360, average train loss: 0.424\n",
      "10:38:30 INFO:iteration 1099, testing\n",
      "10:38:30 INFO:average test loss: 1.800\n",
      "10:38:30 INFO:iteration 1109, train loss: 0.392, average train loss: 0.421\n",
      "10:38:30 INFO:iteration 1119, train loss: 0.348, average train loss: 0.416\n",
      "10:38:31 INFO:iteration 1129, train loss: 0.386, average train loss: 0.408\n",
      "10:38:31 INFO:iteration 1139, train loss: 0.443, average train loss: 0.405\n",
      "10:38:31 INFO:iteration 1149, train loss: 0.507, average train loss: 0.398\n",
      "10:38:31 INFO:iteration 1159, train loss: 0.310, average train loss: 0.394\n",
      "10:38:31 INFO:iteration 1169, train loss: 0.483, average train loss: 0.399\n",
      "10:38:31 INFO:iteration 1179, train loss: 0.427, average train loss: 0.400\n",
      "10:38:31 INFO:iteration 1189, train loss: 0.349, average train loss: 0.396\n",
      "10:38:31 INFO:iteration 1199, train loss: 0.379, average train loss: 0.395\n",
      "10:38:31 INFO:iteration 1199, testing\n",
      "10:38:31 INFO:average test loss: 1.681\n",
      "10:38:32 INFO:iteration 1209, train loss: 0.309, average train loss: 0.389\n",
      "10:38:32 INFO:iteration 1219, train loss: 0.417, average train loss: 0.389\n",
      "10:38:32 INFO:iteration 1229, train loss: 0.347, average train loss: 0.391\n",
      "10:38:32 INFO:iteration 1239, train loss: 0.358, average train loss: 0.386\n",
      "10:38:32 INFO:iteration 1249, train loss: 0.427, average train loss: 0.383\n",
      "10:38:32 INFO:iteration 1259, train loss: 0.281, average train loss: 0.381\n",
      "10:38:32 INFO:iteration 1269, train loss: 0.326, average train loss: 0.375\n",
      "10:38:32 INFO:iteration 1279, train loss: 0.373, average train loss: 0.373\n",
      "10:38:33 INFO:iteration 1289, train loss: 0.375, average train loss: 0.372\n",
      "10:38:33 INFO:iteration 1299, train loss: 0.354, average train loss: 0.369\n",
      "10:38:33 INFO:iteration 1299, testing\n",
      "10:38:33 INFO:average test loss: 1.648\n",
      "10:38:33 INFO:iteration 1309, train loss: 0.363, average train loss: 0.372\n",
      "10:38:33 INFO:iteration 1319, train loss: 0.335, average train loss: 0.370\n",
      "10:38:33 INFO:iteration 1329, train loss: 0.357, average train loss: 0.369\n",
      "10:38:33 INFO:iteration 1339, train loss: 0.357, average train loss: 0.369\n",
      "10:38:33 INFO:iteration 1349, train loss: 0.361, average train loss: 0.366\n",
      "10:38:33 INFO:iteration 1359, train loss: 0.315, average train loss: 0.366\n",
      "10:38:34 INFO:iteration 1369, train loss: 0.382, average train loss: 0.365\n",
      "10:38:34 INFO:iteration 1379, train loss: 0.311, average train loss: 0.361\n",
      "10:38:34 INFO:iteration 1389, train loss: 0.319, average train loss: 0.358\n",
      "10:38:34 INFO:iteration 1399, train loss: 0.291, average train loss: 0.355\n",
      "10:38:34 INFO:iteration 1399, testing\n",
      "10:38:34 INFO:average test loss: 1.671\n",
      "10:38:34 INFO:iteration 1409, train loss: 0.387, average train loss: 0.354\n",
      "10:38:34 INFO:iteration 1419, train loss: 0.333, average train loss: 0.352\n",
      "10:38:34 INFO:iteration 1429, train loss: 0.310, average train loss: 0.348\n",
      "10:38:34 INFO:iteration 1439, train loss: 0.357, average train loss: 0.348\n",
      "10:38:35 INFO:iteration 1449, train loss: 0.350, average train loss: 0.346\n",
      "10:38:35 INFO:iteration 1459, train loss: 0.317, average train loss: 0.346\n",
      "10:38:35 INFO:iteration 1469, train loss: 0.348, average train loss: 0.346\n",
      "10:38:35 INFO:iteration 1479, train loss: 0.336, average train loss: 0.348\n",
      "10:38:35 INFO:iteration 1489, train loss: 0.343, average train loss: 0.349\n",
      "10:38:35 INFO:iteration 1499, train loss: 0.330, average train loss: 0.349\n",
      "10:38:35 INFO:iteration 1499, testing\n",
      "10:38:35 INFO:average test loss: 1.719\n",
      "10:38:35 INFO:iteration 1509, train loss: 0.277, average train loss: 0.347\n",
      "10:38:36 INFO:iteration 1519, train loss: 0.337, average train loss: 0.347\n",
      "10:38:36 INFO:iteration 1529, train loss: 0.339, average train loss: 0.345\n",
      "10:38:36 INFO:iteration 1539, train loss: 0.276, average train loss: 0.344\n",
      "10:38:36 INFO:iteration 1549, train loss: 0.375, average train loss: 0.343\n",
      "10:38:36 INFO:iteration 1559, train loss: 0.342, average train loss: 0.344\n",
      "10:38:36 INFO:iteration 1569, train loss: 0.358, average train loss: 0.343\n",
      "10:38:36 INFO:iteration 1579, train loss: 0.323, average train loss: 0.342\n",
      "10:38:36 INFO:iteration 1589, train loss: 0.323, average train loss: 0.343\n",
      "10:38:36 INFO:iteration 1599, train loss: 0.336, average train loss: 0.344\n",
      "10:38:36 INFO:iteration 1599, testing\n",
      "10:38:37 INFO:average test loss: 1.636\n",
      "10:38:37 INFO:iteration 1609, train loss: 0.366, average train loss: 0.344\n",
      "10:38:37 INFO:iteration 1619, train loss: 0.308, average train loss: 0.343\n",
      "10:38:37 INFO:iteration 1629, train loss: 0.280, average train loss: 0.342\n",
      "10:38:37 INFO:iteration 1639, train loss: 0.275, average train loss: 0.340\n",
      "10:38:37 INFO:iteration 1649, train loss: 0.371, average train loss: 0.341\n",
      "10:38:37 INFO:iteration 1659, train loss: 0.333, average train loss: 0.336\n",
      "10:38:37 INFO:iteration 1669, train loss: 0.400, average train loss: 0.332\n",
      "10:38:37 INFO:iteration 1679, train loss: 0.304, average train loss: 0.329\n",
      "10:38:38 INFO:iteration 1689, train loss: 0.254, average train loss: 0.323\n",
      "10:38:38 INFO:iteration 1699, train loss: 0.326, average train loss: 0.322\n",
      "10:38:38 INFO:iteration 1699, testing\n",
      "10:38:38 INFO:average test loss: 1.675\n",
      "10:38:38 INFO:iteration 1709, train loss: 0.468, average train loss: 0.320\n",
      "10:38:38 INFO:iteration 1719, train loss: 0.327, average train loss: 0.321\n",
      "10:38:38 INFO:iteration 1729, train loss: 0.282, average train loss: 0.325\n",
      "10:38:38 INFO:iteration 1739, train loss: 0.338, average train loss: 0.328\n",
      "10:38:38 INFO:iteration 1749, train loss: 0.295, average train loss: 0.330\n",
      "10:38:39 INFO:iteration 1759, train loss: 0.304, average train loss: 0.334\n",
      "10:38:39 INFO:iteration 1769, train loss: 0.328, average train loss: 0.336\n",
      "10:38:39 INFO:iteration 1779, train loss: 0.354, average train loss: 0.336\n",
      "10:38:39 INFO:iteration 1789, train loss: 0.244, average train loss: 0.337\n",
      "10:38:39 INFO:iteration 1799, train loss: 0.330, average train loss: 0.337\n",
      "10:38:39 INFO:iteration 1799, testing\n",
      "10:38:39 INFO:average test loss: 1.728\n",
      "10:38:39 INFO:iteration 1809, train loss: 0.285, average train loss: 0.335\n",
      "10:38:39 INFO:iteration 1819, train loss: 0.286, average train loss: 0.333\n",
      "10:38:39 INFO:iteration 1829, train loss: 0.308, average train loss: 0.331\n",
      "10:38:40 INFO:iteration 1839, train loss: 0.299, average train loss: 0.328\n",
      "10:38:40 INFO:iteration 1849, train loss: 0.361, average train loss: 0.324\n",
      "10:38:40 INFO:iteration 1859, train loss: 0.329, average train loss: 0.319\n",
      "10:38:40 INFO:iteration 1869, train loss: 0.352, average train loss: 0.316\n",
      "10:38:40 INFO:iteration 1879, train loss: 0.284, average train loss: 0.314\n",
      "10:38:40 INFO:iteration 1889, train loss: 0.372, average train loss: 0.313\n",
      "10:38:40 INFO:iteration 1899, train loss: 0.332, average train loss: 0.312\n",
      "10:38:40 INFO:iteration 1899, testing\n",
      "10:38:40 INFO:average test loss: 1.606\n",
      "10:38:40 INFO:iteration 1909, train loss: 0.345, average train loss: 0.312\n",
      "10:38:41 INFO:iteration 1919, train loss: 0.300, average train loss: 0.312\n",
      "10:38:41 INFO:iteration 1929, train loss: 0.357, average train loss: 0.311\n",
      "10:38:41 INFO:iteration 1939, train loss: 0.273, average train loss: 0.310\n",
      "10:38:41 INFO:iteration 1949, train loss: 0.269, average train loss: 0.310\n",
      "10:38:41 INFO:iteration 1959, train loss: 0.280, average train loss: 0.311\n",
      "10:38:41 INFO:iteration 1969, train loss: 0.303, average train loss: 0.310\n",
      "10:38:41 INFO:iteration 1979, train loss: 0.313, average train loss: 0.311\n",
      "10:38:41 INFO:iteration 1989, train loss: 0.317, average train loss: 0.311\n",
      "10:38:42 INFO:iteration 1999, train loss: 0.318, average train loss: 0.311\n",
      "10:38:42 INFO:iteration 1999, testing\n",
      "10:38:42 INFO:average test loss: 1.672\n",
      "10:38:42 INFO:iteration 2001, saving model\n",
      "10:38:42 INFO:iteration 2009, train loss: 0.275, average train loss: 0.308\n",
      "10:38:42 INFO:iteration 2019, train loss: 0.288, average train loss: 0.307\n",
      "10:38:42 INFO:iteration 2029, train loss: 0.283, average train loss: 0.303\n",
      "10:38:42 INFO:iteration 2039, train loss: 0.272, average train loss: 0.302\n",
      "10:38:42 INFO:iteration 2049, train loss: 0.272, average train loss: 0.302\n",
      "10:38:42 INFO:iteration 2059, train loss: 0.246, average train loss: 0.303\n",
      "10:38:43 INFO:iteration 2069, train loss: 0.268, average train loss: 0.303\n",
      "10:38:43 INFO:iteration 2079, train loss: 0.290, average train loss: 0.303\n",
      "10:38:43 INFO:iteration 2089, train loss: 0.295, average train loss: 0.303\n",
      "10:38:43 INFO:iteration 2099, train loss: 0.257, average train loss: 0.300\n",
      "10:38:43 INFO:iteration 2099, testing\n",
      "10:38:43 INFO:average test loss: 1.656\n",
      "10:38:43 INFO:iteration 2109, train loss: 0.313, average train loss: 0.302\n",
      "10:38:43 INFO:iteration 2119, train loss: 0.269, average train loss: 0.300\n",
      "10:38:43 INFO:iteration 2129, train loss: 0.443, average train loss: 0.303\n",
      "10:38:43 INFO:iteration 2139, train loss: 0.262, average train loss: 0.302\n",
      "10:38:44 INFO:iteration 2149, train loss: 0.287, average train loss: 0.302\n",
      "10:38:44 INFO:iteration 2159, train loss: 0.303, average train loss: 0.298\n",
      "10:38:44 INFO:iteration 2169, train loss: 0.290, average train loss: 0.299\n",
      "10:38:44 INFO:iteration 2179, train loss: 0.275, average train loss: 0.299\n",
      "10:38:44 INFO:iteration 2189, train loss: 0.331, average train loss: 0.297\n",
      "10:38:44 INFO:iteration 2199, train loss: 0.317, average train loss: 0.296\n",
      "10:38:44 INFO:iteration 2199, testing\n",
      "10:38:44 INFO:average test loss: 1.611\n",
      "10:38:44 INFO:iteration 2209, train loss: 0.322, average train loss: 0.297\n",
      "10:38:45 INFO:iteration 2219, train loss: 0.261, average train loss: 0.297\n",
      "10:38:45 INFO:iteration 2229, train loss: 0.338, average train loss: 0.295\n",
      "10:38:45 INFO:iteration 2239, train loss: 0.245, average train loss: 0.297\n",
      "10:38:45 INFO:iteration 2249, train loss: 0.295, average train loss: 0.295\n",
      "10:38:45 INFO:iteration 2259, train loss: 0.291, average train loss: 0.294\n",
      "10:38:45 INFO:iteration 2269, train loss: 0.249, average train loss: 0.294\n",
      "10:38:45 INFO:iteration 2279, train loss: 0.310, average train loss: 0.291\n",
      "10:38:45 INFO:iteration 2289, train loss: 0.287, average train loss: 0.289\n",
      "10:38:46 INFO:iteration 2299, train loss: 0.282, average train loss: 0.291\n",
      "10:38:46 INFO:iteration 2299, testing\n",
      "10:38:46 INFO:average test loss: 1.612\n",
      "10:38:46 INFO:iteration 2309, train loss: 0.282, average train loss: 0.293\n",
      "10:38:46 INFO:iteration 2319, train loss: 0.272, average train loss: 0.291\n",
      "10:38:46 INFO:iteration 2329, train loss: 0.272, average train loss: 0.289\n",
      "10:38:46 INFO:iteration 2339, train loss: 0.265, average train loss: 0.287\n",
      "10:38:46 INFO:iteration 2349, train loss: 0.318, average train loss: 0.285\n",
      "10:38:46 INFO:iteration 2359, train loss: 0.346, average train loss: 0.290\n",
      "10:38:47 INFO:iteration 2369, train loss: 0.260, average train loss: 0.289\n",
      "10:38:47 INFO:iteration 2379, train loss: 0.336, average train loss: 0.293\n",
      "10:38:47 INFO:iteration 2389, train loss: 0.297, average train loss: 0.293\n",
      "10:38:47 INFO:iteration 2399, train loss: 0.260, average train loss: 0.290\n",
      "10:38:47 INFO:iteration 2399, testing\n",
      "10:38:47 INFO:average test loss: 1.625\n",
      "10:38:47 INFO:iteration 2409, train loss: 0.242, average train loss: 0.284\n",
      "10:38:47 INFO:iteration 2419, train loss: 0.318, average train loss: 0.283\n",
      "10:38:47 INFO:iteration 2429, train loss: 0.225, average train loss: 0.283\n",
      "10:38:47 INFO:iteration 2439, train loss: 0.276, average train loss: 0.283\n",
      "10:38:48 INFO:iteration 2449, train loss: 0.279, average train loss: 0.280\n",
      "10:38:48 INFO:iteration 2459, train loss: 0.252, average train loss: 0.276\n",
      "10:38:48 INFO:iteration 2469, train loss: 0.260, average train loss: 0.277\n",
      "10:38:48 INFO:iteration 2479, train loss: 0.261, average train loss: 0.271\n",
      "10:38:48 INFO:iteration 2489, train loss: 0.276, average train loss: 0.270\n",
      "10:38:48 INFO:iteration 2499, train loss: 0.320, average train loss: 0.275\n",
      "10:38:48 INFO:iteration 2499, testing\n",
      "10:38:48 INFO:average test loss: 1.695\n",
      "10:38:48 INFO:iteration 2509, train loss: 0.260, average train loss: 0.277\n",
      "10:38:48 INFO:iteration 2519, train loss: 0.254, average train loss: 0.275\n",
      "10:38:49 INFO:iteration 2529, train loss: 0.274, average train loss: 0.275\n",
      "10:38:49 INFO:iteration 2539, train loss: 0.269, average train loss: 0.274\n",
      "10:38:49 INFO:iteration 2549, train loss: 0.281, average train loss: 0.277\n",
      "10:38:49 INFO:iteration 2559, train loss: 0.317, average train loss: 0.278\n",
      "10:38:49 INFO:iteration 2569, train loss: 0.282, average train loss: 0.278\n",
      "10:38:49 INFO:iteration 2579, train loss: 0.248, average train loss: 0.279\n",
      "10:38:49 INFO:iteration 2589, train loss: 0.230, average train loss: 0.283\n",
      "10:38:49 INFO:iteration 2599, train loss: 0.252, average train loss: 0.280\n",
      "10:38:49 INFO:iteration 2599, testing\n",
      "10:38:49 INFO:average test loss: 1.629\n",
      "10:38:50 INFO:iteration 2609, train loss: 0.565, average train loss: 0.280\n",
      "10:38:50 INFO:iteration 2619, train loss: 0.330, average train loss: 0.282\n",
      "10:38:50 INFO:iteration 2629, train loss: 0.291, average train loss: 0.283\n",
      "10:38:50 INFO:iteration 2639, train loss: 0.221, average train loss: 0.282\n",
      "10:38:50 INFO:iteration 2649, train loss: 0.224, average train loss: 0.279\n",
      "10:38:50 INFO:iteration 2659, train loss: 0.238, average train loss: 0.276\n",
      "10:38:50 INFO:iteration 2669, train loss: 0.257, average train loss: 0.273\n",
      "10:38:50 INFO:iteration 2679, train loss: 0.214, average train loss: 0.271\n",
      "10:38:51 INFO:iteration 2689, train loss: 0.300, average train loss: 0.268\n",
      "10:38:51 INFO:iteration 2699, train loss: 0.251, average train loss: 0.268\n",
      "10:38:51 INFO:iteration 2699, testing\n",
      "10:38:51 INFO:average test loss: 1.648\n",
      "10:38:51 INFO:iteration 2709, train loss: 0.250, average train loss: 0.265\n",
      "10:38:51 INFO:iteration 2719, train loss: 0.306, average train loss: 0.267\n",
      "10:38:51 INFO:iteration 2729, train loss: 0.257, average train loss: 0.265\n",
      "10:38:51 INFO:iteration 2739, train loss: 0.303, average train loss: 0.265\n",
      "10:38:51 INFO:iteration 2749, train loss: 0.270, average train loss: 0.268\n",
      "10:38:51 INFO:iteration 2759, train loss: 0.275, average train loss: 0.269\n",
      "10:38:52 INFO:iteration 2769, train loss: 0.290, average train loss: 0.269\n",
      "10:38:52 INFO:iteration 2779, train loss: 0.267, average train loss: 0.272\n",
      "10:38:52 INFO:iteration 2789, train loss: 0.299, average train loss: 0.272\n",
      "10:38:52 INFO:iteration 2799, train loss: 0.368, average train loss: 0.277\n",
      "10:38:52 INFO:iteration 2799, testing\n",
      "10:38:52 INFO:average test loss: 1.676\n",
      "10:38:52 INFO:iteration 2809, train loss: 0.290, average train loss: 0.279\n",
      "10:38:52 INFO:iteration 2819, train loss: 0.239, average train loss: 0.277\n",
      "10:38:52 INFO:iteration 2829, train loss: 0.356, average train loss: 0.278\n",
      "10:38:52 INFO:iteration 2839, train loss: 0.253, average train loss: 0.279\n",
      "10:38:53 INFO:iteration 2849, train loss: 0.287, average train loss: 0.276\n",
      "10:38:53 INFO:iteration 2859, train loss: 0.288, average train loss: 0.277\n",
      "10:38:53 INFO:iteration 2869, train loss: 0.248, average train loss: 0.276\n",
      "10:38:53 INFO:iteration 2879, train loss: 0.339, average train loss: 0.276\n",
      "10:38:53 INFO:iteration 2889, train loss: 0.318, average train loss: 0.274\n",
      "10:38:53 INFO:iteration 2899, train loss: 0.194, average train loss: 0.268\n",
      "10:38:53 INFO:iteration 2899, testing\n",
      "10:38:53 INFO:average test loss: 1.706\n",
      "10:38:53 INFO:iteration 2909, train loss: 0.257, average train loss: 0.268\n",
      "10:38:54 INFO:iteration 2919, train loss: 0.237, average train loss: 0.267\n",
      "10:38:54 INFO:iteration 2929, train loss: 0.244, average train loss: 0.265\n",
      "10:38:54 INFO:iteration 2939, train loss: 0.243, average train loss: 0.263\n",
      "10:38:54 INFO:iteration 2949, train loss: 0.245, average train loss: 0.262\n",
      "10:38:54 INFO:iteration 2959, train loss: 0.218, average train loss: 0.261\n",
      "10:38:54 INFO:iteration 2969, train loss: 0.237, average train loss: 0.259\n",
      "10:38:54 INFO:iteration 2979, train loss: 0.250, average train loss: 0.257\n",
      "10:38:54 INFO:iteration 2989, train loss: 0.246, average train loss: 0.255\n",
      "10:38:54 INFO:iteration 2999, train loss: 0.300, average train loss: 0.254\n",
      "10:38:54 INFO:iteration 2999, testing\n",
      "10:38:55 INFO:average test loss: 1.555\n",
      "10:38:55 INFO:iteration 3002, saving model\n",
      "10:38:55 INFO:iteration 3009, train loss: 0.300, average train loss: 0.254\n",
      "10:38:55 INFO:iteration 3019, train loss: 0.395, average train loss: 0.257\n",
      "10:38:55 INFO:iteration 3029, train loss: 0.252, average train loss: 0.257\n",
      "10:38:55 INFO:iteration 3039, train loss: 0.205, average train loss: 0.258\n",
      "10:38:55 INFO:iteration 3049, train loss: 0.223, average train loss: 0.260\n",
      "10:38:55 INFO:iteration 3059, train loss: 0.227, average train loss: 0.260\n",
      "10:38:55 INFO:iteration 3069, train loss: 0.329, average train loss: 0.261\n",
      "10:38:55 INFO:iteration 3079, train loss: 0.226, average train loss: 0.262\n",
      "10:38:56 INFO:iteration 3089, train loss: 0.242, average train loss: 0.263\n",
      "10:38:56 INFO:iteration 3099, train loss: 0.374, average train loss: 0.262\n",
      "10:38:56 INFO:iteration 3099, testing\n",
      "10:38:56 INFO:average test loss: 1.543\n",
      "10:38:56 INFO:iteration 3109, train loss: 0.324, average train loss: 0.261\n",
      "10:38:56 INFO:iteration 3119, train loss: 0.263, average train loss: 0.262\n",
      "10:38:56 INFO:iteration 3129, train loss: 0.226, average train loss: 0.260\n",
      "10:38:56 INFO:iteration 3139, train loss: 0.227, average train loss: 0.262\n",
      "10:38:56 INFO:iteration 3149, train loss: 0.264, average train loss: 0.259\n",
      "10:38:56 INFO:iteration 3159, train loss: 0.239, average train loss: 0.260\n",
      "10:38:57 INFO:iteration 3169, train loss: 0.326, average train loss: 0.263\n",
      "10:38:57 INFO:iteration 3179, train loss: 0.237, average train loss: 0.264\n",
      "10:38:57 INFO:iteration 3189, train loss: 0.234, average train loss: 0.266\n",
      "10:38:57 INFO:iteration 3199, train loss: 0.234, average train loss: 0.266\n",
      "10:38:57 INFO:iteration 3199, testing\n",
      "10:38:57 INFO:average test loss: 1.580\n",
      "10:38:57 INFO:iteration 3209, train loss: 0.221, average train loss: 0.263\n",
      "10:38:57 INFO:iteration 3219, train loss: 0.258, average train loss: 0.262\n",
      "10:38:57 INFO:iteration 3229, train loss: 0.220, average train loss: 0.262\n",
      "10:38:58 INFO:iteration 3239, train loss: 0.218, average train loss: 0.260\n",
      "10:38:58 INFO:iteration 3249, train loss: 0.218, average train loss: 0.261\n",
      "10:38:58 INFO:iteration 3259, train loss: 0.210, average train loss: 0.259\n",
      "10:38:58 INFO:iteration 3269, train loss: 0.204, average train loss: 0.254\n",
      "10:38:58 INFO:iteration 3279, train loss: 0.234, average train loss: 0.251\n",
      "10:38:58 INFO:iteration 3289, train loss: 0.282, average train loss: 0.250\n",
      "10:38:58 INFO:iteration 3299, train loss: 0.235, average train loss: 0.250\n",
      "10:38:58 INFO:iteration 3299, testing\n",
      "10:38:58 INFO:average test loss: 1.634\n",
      "10:38:58 INFO:iteration 3309, train loss: 0.264, average train loss: 0.253\n",
      "10:38:59 INFO:iteration 3319, train loss: 0.211, average train loss: 0.248\n",
      "10:38:59 INFO:iteration 3329, train loss: 0.227, average train loss: 0.249\n",
      "10:38:59 INFO:iteration 3339, train loss: 0.244, average train loss: 0.248\n",
      "10:38:59 INFO:iteration 3349, train loss: 0.275, average train loss: 0.252\n",
      "10:38:59 INFO:iteration 3359, train loss: 0.247, average train loss: 0.253\n",
      "10:38:59 INFO:iteration 3369, train loss: 0.271, average train loss: 0.254\n",
      "10:38:59 INFO:iteration 3379, train loss: 0.241, average train loss: 0.254\n",
      "10:38:59 INFO:iteration 3389, train loss: 0.248, average train loss: 0.255\n",
      "10:39:00 INFO:iteration 3399, train loss: 0.218, average train loss: 0.253\n",
      "10:39:00 INFO:iteration 3399, testing\n",
      "10:39:00 INFO:average test loss: 1.557\n",
      "10:39:00 INFO:iteration 3409, train loss: 0.220, average train loss: 0.253\n",
      "10:39:00 INFO:iteration 3419, train loss: 0.281, average train loss: 0.254\n",
      "10:39:00 INFO:iteration 3429, train loss: 0.264, average train loss: 0.251\n",
      "10:39:00 INFO:iteration 3439, train loss: 0.272, average train loss: 0.250\n",
      "10:39:00 INFO:iteration 3449, train loss: 0.352, average train loss: 0.253\n",
      "10:39:00 INFO:iteration 3459, train loss: 0.248, average train loss: 0.253\n",
      "10:39:00 INFO:iteration 3469, train loss: 0.280, average train loss: 0.254\n",
      "10:39:01 INFO:iteration 3479, train loss: 0.297, average train loss: 0.255\n",
      "10:39:01 INFO:iteration 3489, train loss: 0.316, average train loss: 0.256\n",
      "10:39:01 INFO:iteration 3499, train loss: 0.251, average train loss: 0.257\n",
      "10:39:01 INFO:iteration 3499, testing\n",
      "10:39:01 INFO:average test loss: 1.552\n",
      "10:39:01 INFO:iteration 3509, train loss: 0.262, average train loss: 0.257\n",
      "10:39:01 INFO:iteration 3519, train loss: 0.231, average train loss: 0.261\n",
      "10:39:01 INFO:iteration 3529, train loss: 0.225, average train loss: 0.266\n",
      "10:39:01 INFO:iteration 3539, train loss: 0.248, average train loss: 0.267\n",
      "10:39:01 INFO:iteration 3549, train loss: 0.176, average train loss: 0.262\n",
      "10:39:02 INFO:iteration 3559, train loss: 0.225, average train loss: 0.258\n",
      "10:39:02 INFO:iteration 3569, train loss: 0.211, average train loss: 0.256\n",
      "10:39:02 INFO:iteration 3579, train loss: 0.225, average train loss: 0.253\n",
      "10:39:02 INFO:iteration 3589, train loss: 0.229, average train loss: 0.251\n",
      "10:39:02 INFO:iteration 3599, train loss: 0.214, average train loss: 0.251\n",
      "10:39:02 INFO:iteration 3599, testing\n",
      "10:39:02 INFO:average test loss: 1.608\n",
      "10:39:02 INFO:iteration 3609, train loss: 0.241, average train loss: 0.249\n",
      "10:39:02 INFO:iteration 3619, train loss: 0.199, average train loss: 0.245\n",
      "10:39:03 INFO:iteration 3629, train loss: 0.203, average train loss: 0.239\n",
      "10:39:03 INFO:iteration 3639, train loss: 0.213, average train loss: 0.237\n",
      "10:39:03 INFO:iteration 3649, train loss: 0.280, average train loss: 0.236\n",
      "10:39:03 INFO:iteration 3659, train loss: 0.242, average train loss: 0.237\n",
      "10:39:03 INFO:iteration 3669, train loss: 0.228, average train loss: 0.236\n",
      "10:39:03 INFO:iteration 3679, train loss: 0.201, average train loss: 0.238\n",
      "10:39:03 INFO:iteration 3689, train loss: 0.230, average train loss: 0.236\n",
      "10:39:03 INFO:iteration 3699, train loss: 0.196, average train loss: 0.233\n",
      "10:39:03 INFO:iteration 3699, testing\n",
      "10:39:03 INFO:average test loss: 1.622\n",
      "10:39:04 INFO:iteration 3709, train loss: 0.293, average train loss: 0.234\n",
      "10:39:04 INFO:iteration 3719, train loss: 0.210, average train loss: 0.235\n",
      "10:39:04 INFO:iteration 3729, train loss: 0.239, average train loss: 0.236\n",
      "10:39:04 INFO:iteration 3739, train loss: 0.259, average train loss: 0.237\n",
      "10:39:04 INFO:iteration 3749, train loss: 0.253, average train loss: 0.236\n",
      "10:39:04 INFO:iteration 3759, train loss: 0.218, average train loss: 0.236\n",
      "10:39:04 INFO:iteration 3769, train loss: 0.198, average train loss: 0.238\n",
      "10:39:04 INFO:iteration 3779, train loss: 0.261, average train loss: 0.238\n",
      "10:39:05 INFO:iteration 3789, train loss: 0.249, average train loss: 0.242\n",
      "10:39:05 INFO:iteration 3799, train loss: 0.173, average train loss: 0.243\n",
      "10:39:05 INFO:iteration 3799, testing\n",
      "10:39:05 INFO:average test loss: 1.561\n",
      "10:39:05 INFO:iteration 3809, train loss: 0.226, average train loss: 0.242\n",
      "10:39:05 INFO:iteration 3819, train loss: 0.238, average train loss: 0.242\n",
      "10:39:05 INFO:iteration 3829, train loss: 0.429, average train loss: 0.243\n",
      "10:39:05 INFO:iteration 3839, train loss: 0.217, average train loss: 0.244\n",
      "10:39:05 INFO:iteration 3849, train loss: 0.202, average train loss: 0.243\n",
      "10:39:05 INFO:iteration 3859, train loss: 0.201, average train loss: 0.243\n",
      "10:39:06 INFO:iteration 3869, train loss: 0.221, average train loss: 0.243\n",
      "10:39:06 INFO:iteration 3879, train loss: 0.212, average train loss: 0.241\n",
      "10:39:06 INFO:iteration 3889, train loss: 0.210, average train loss: 0.241\n",
      "10:39:06 INFO:iteration 3899, train loss: 0.298, average train loss: 0.247\n",
      "10:39:06 INFO:iteration 3899, testing\n",
      "10:39:06 INFO:average test loss: 1.614\n",
      "10:39:06 INFO:iteration 3909, train loss: 0.213, average train loss: 0.247\n",
      "10:39:06 INFO:iteration 3919, train loss: 0.231, average train loss: 0.248\n",
      "10:39:06 INFO:iteration 3929, train loss: 0.208, average train loss: 0.249\n",
      "10:39:06 INFO:iteration 3939, train loss: 0.221, average train loss: 0.248\n",
      "10:39:07 INFO:iteration 3949, train loss: 0.204, average train loss: 0.249\n",
      "10:39:07 INFO:iteration 3959, train loss: 0.199, average train loss: 0.251\n",
      "10:39:07 INFO:iteration 3969, train loss: 0.218, average train loss: 0.255\n",
      "10:39:07 INFO:iteration 3979, train loss: 0.262, average train loss: 0.255\n",
      "10:39:07 INFO:iteration 3989, train loss: 0.245, average train loss: 0.254\n",
      "10:39:07 INFO:iteration 3999, train loss: 0.246, average train loss: 0.249\n",
      "10:39:07 INFO:iteration 3999, testing\n",
      "10:39:07 INFO:average test loss: 1.602\n",
      "10:39:07 INFO:iteration 4003, saving model\n",
      "10:39:07 INFO:iteration 4009, train loss: 0.242, average train loss: 0.249\n",
      "10:39:08 INFO:iteration 4019, train loss: 0.172, average train loss: 0.246\n",
      "10:39:08 INFO:iteration 4029, train loss: 0.213, average train loss: 0.243\n",
      "10:39:08 INFO:iteration 4039, train loss: 0.237, average train loss: 0.240\n",
      "10:39:08 INFO:iteration 4049, train loss: 0.295, average train loss: 0.240\n",
      "10:39:08 INFO:iteration 4059, train loss: 0.313, average train loss: 0.239\n",
      "10:39:08 INFO:iteration 4069, train loss: 0.194, average train loss: 0.233\n",
      "10:39:08 INFO:iteration 4079, train loss: 0.281, average train loss: 0.233\n",
      "10:39:08 INFO:iteration 4089, train loss: 0.234, average train loss: 0.231\n",
      "10:39:08 INFO:iteration 4099, train loss: 0.234, average train loss: 0.229\n",
      "10:39:08 INFO:iteration 4099, testing\n",
      "10:39:08 INFO:average test loss: 1.614\n",
      "10:39:09 INFO:iteration 4109, train loss: 0.220, average train loss: 0.227\n",
      "10:39:09 INFO:iteration 4119, train loss: 0.185, average train loss: 0.230\n",
      "10:39:09 INFO:iteration 4129, train loss: 0.266, average train loss: 0.233\n",
      "10:39:09 INFO:iteration 4139, train loss: 0.214, average train loss: 0.233\n",
      "10:39:09 INFO:iteration 4149, train loss: 0.185, average train loss: 0.231\n",
      "10:39:09 INFO:iteration 4159, train loss: 0.288, average train loss: 0.232\n",
      "10:39:09 INFO:iteration 4169, train loss: 0.200, average train loss: 0.230\n",
      "10:39:09 INFO:iteration 4179, train loss: 0.218, average train loss: 0.230\n",
      "10:39:10 INFO:iteration 4189, train loss: 0.208, average train loss: 0.230\n",
      "10:39:10 INFO:iteration 4199, train loss: 0.258, average train loss: 0.233\n",
      "10:39:10 INFO:iteration 4199, testing\n",
      "10:39:10 INFO:average test loss: 1.540\n",
      "10:39:10 INFO:iteration 4209, train loss: 0.205, average train loss: 0.232\n",
      "10:39:10 INFO:iteration 4219, train loss: 0.324, average train loss: 0.232\n",
      "10:39:10 INFO:iteration 4229, train loss: 0.192, average train loss: 0.230\n",
      "10:39:10 INFO:iteration 4239, train loss: 0.306, average train loss: 0.231\n",
      "10:39:10 INFO:iteration 4249, train loss: 0.197, average train loss: 0.232\n",
      "10:39:10 INFO:iteration 4259, train loss: 0.212, average train loss: 0.230\n",
      "10:39:10 INFO:iteration 4269, train loss: 0.260, average train loss: 0.233\n",
      "10:39:11 INFO:iteration 4279, train loss: 0.200, average train loss: 0.232\n",
      "10:39:11 INFO:iteration 4289, train loss: 0.226, average train loss: 0.231\n",
      "10:39:11 INFO:iteration 4299, train loss: 0.205, average train loss: 0.228\n",
      "10:39:11 INFO:iteration 4299, testing\n",
      "10:39:11 INFO:average test loss: 1.551\n",
      "10:39:11 INFO:iteration 4309, train loss: 0.206, average train loss: 0.231\n",
      "10:39:11 INFO:iteration 4319, train loss: 0.254, average train loss: 0.230\n",
      "10:39:11 INFO:iteration 4329, train loss: 0.216, average train loss: 0.231\n",
      "10:39:11 INFO:iteration 4339, train loss: 0.306, average train loss: 0.232\n",
      "10:39:11 INFO:iteration 4349, train loss: 0.259, average train loss: 0.232\n",
      "10:39:12 INFO:iteration 4359, train loss: 0.202, average train loss: 0.232\n",
      "10:39:12 INFO:iteration 4369, train loss: 0.213, average train loss: 0.231\n",
      "10:39:12 INFO:iteration 4379, train loss: 0.224, average train loss: 0.233\n",
      "10:39:12 INFO:iteration 4389, train loss: 0.214, average train loss: 0.235\n",
      "10:39:12 INFO:iteration 4399, train loss: 0.257, average train loss: 0.235\n",
      "10:39:12 INFO:iteration 4399, testing\n",
      "10:39:12 INFO:average test loss: 1.606\n",
      "10:39:12 INFO:iteration 4409, train loss: 0.362, average train loss: 0.234\n",
      "10:39:12 INFO:iteration 4419, train loss: 0.207, average train loss: 0.230\n",
      "10:39:13 INFO:iteration 4429, train loss: 0.235, average train loss: 0.232\n",
      "10:39:13 INFO:iteration 4439, train loss: 0.243, average train loss: 0.231\n",
      "10:39:13 INFO:iteration 4449, train loss: 0.239, average train loss: 0.231\n",
      "10:39:13 INFO:iteration 4459, train loss: 0.193, average train loss: 0.231\n",
      "10:39:13 INFO:iteration 4469, train loss: 0.195, average train loss: 0.232\n",
      "10:39:13 INFO:iteration 4479, train loss: 0.338, average train loss: 0.232\n",
      "10:39:13 INFO:iteration 4489, train loss: 0.222, average train loss: 0.231\n",
      "10:39:13 INFO:iteration 4499, train loss: 0.189, average train loss: 0.233\n",
      "10:39:13 INFO:iteration 4499, testing\n",
      "10:39:13 INFO:average test loss: 1.584\n",
      "10:39:14 INFO:iteration 4509, train loss: 0.209, average train loss: 0.233\n",
      "10:39:14 INFO:iteration 4519, train loss: 0.431, average train loss: 0.237\n",
      "10:39:14 INFO:iteration 4529, train loss: 0.199, average train loss: 0.236\n",
      "10:39:14 INFO:iteration 4539, train loss: 0.214, average train loss: 0.237\n",
      "10:39:14 INFO:iteration 4549, train loss: 0.192, average train loss: 0.237\n",
      "10:39:14 INFO:iteration 4559, train loss: 0.181, average train loss: 0.238\n",
      "10:39:14 INFO:iteration 4569, train loss: 0.177, average train loss: 0.238\n",
      "10:39:14 INFO:iteration 4579, train loss: 0.210, average train loss: 0.237\n",
      "10:39:14 INFO:iteration 4589, train loss: 0.214, average train loss: 0.236\n",
      "10:39:15 INFO:iteration 4599, train loss: 0.179, average train loss: 0.236\n",
      "10:39:15 INFO:iteration 4599, testing\n",
      "10:39:15 INFO:average test loss: 1.581\n",
      "10:39:15 INFO:iteration 4609, train loss: 0.414, average train loss: 0.239\n",
      "10:39:15 INFO:iteration 4619, train loss: 0.230, average train loss: 0.237\n",
      "10:39:15 INFO:iteration 4629, train loss: 0.220, average train loss: 0.235\n",
      "10:39:15 INFO:iteration 4639, train loss: 0.216, average train loss: 0.232\n",
      "10:39:15 INFO:iteration 4649, train loss: 0.232, average train loss: 0.232\n",
      "10:39:15 INFO:iteration 4659, train loss: 0.264, average train loss: 0.232\n",
      "10:39:15 INFO:iteration 4669, train loss: 0.198, average train loss: 0.231\n",
      "10:39:16 INFO:iteration 4679, train loss: 0.229, average train loss: 0.230\n",
      "10:39:16 INFO:iteration 4689, train loss: 0.256, average train loss: 0.231\n",
      "10:39:16 INFO:iteration 4699, train loss: 0.229, average train loss: 0.232\n",
      "10:39:16 INFO:iteration 4699, testing\n",
      "10:39:16 INFO:average test loss: 1.571\n",
      "10:39:16 INFO:iteration 4709, train loss: 0.209, average train loss: 0.228\n",
      "10:39:16 INFO:iteration 4719, train loss: 0.200, average train loss: 0.227\n",
      "10:39:16 INFO:iteration 4729, train loss: 0.256, average train loss: 0.229\n",
      "10:39:16 INFO:iteration 4739, train loss: 0.262, average train loss: 0.232\n",
      "10:39:17 INFO:iteration 4749, train loss: 0.233, average train loss: 0.230\n",
      "10:39:17 INFO:iteration 4759, train loss: 0.182, average train loss: 0.228\n",
      "10:39:17 INFO:iteration 4769, train loss: 0.212, average train loss: 0.226\n",
      "10:39:17 INFO:iteration 4779, train loss: 0.266, average train loss: 0.227\n",
      "10:39:17 INFO:iteration 4789, train loss: 0.212, average train loss: 0.227\n",
      "10:39:17 INFO:iteration 4799, train loss: 0.246, average train loss: 0.226\n",
      "10:39:17 INFO:iteration 4799, testing\n",
      "10:39:17 INFO:average test loss: 1.591\n",
      "10:39:17 INFO:iteration 4809, train loss: 0.184, average train loss: 0.226\n",
      "10:39:17 INFO:iteration 4819, train loss: 0.359, average train loss: 0.225\n",
      "10:39:18 INFO:iteration 4829, train loss: 0.205, average train loss: 0.222\n",
      "10:39:18 INFO:iteration 4839, train loss: 0.218, average train loss: 0.222\n",
      "10:39:18 INFO:iteration 4849, train loss: 0.186, average train loss: 0.223\n",
      "10:39:18 INFO:iteration 4859, train loss: 0.365, average train loss: 0.226\n",
      "10:39:18 INFO:iteration 4869, train loss: 0.228, average train loss: 0.226\n",
      "10:39:18 INFO:iteration 4879, train loss: 0.214, average train loss: 0.226\n",
      "10:39:18 INFO:iteration 4889, train loss: 0.183, average train loss: 0.225\n",
      "10:39:18 INFO:iteration 4899, train loss: 0.181, average train loss: 0.222\n",
      "10:39:18 INFO:iteration 4899, testing\n",
      "10:39:18 INFO:average test loss: 1.598\n",
      "10:39:19 INFO:iteration 4909, train loss: 0.192, average train loss: 0.225\n",
      "10:39:19 INFO:iteration 4919, train loss: 0.200, average train loss: 0.226\n",
      "10:39:19 INFO:iteration 4929, train loss: 0.264, average train loss: 0.227\n",
      "10:39:19 INFO:iteration 4939, train loss: 0.185, average train loss: 0.224\n",
      "10:39:19 INFO:iteration 4949, train loss: 0.287, average train loss: 0.223\n",
      "10:39:19 INFO:iteration 4959, train loss: 0.243, average train loss: 0.220\n",
      "10:39:19 INFO:iteration 4969, train loss: 0.237, average train loss: 0.224\n",
      "10:39:19 INFO:iteration 4979, train loss: 0.210, average train loss: 0.225\n",
      "10:39:19 INFO:iteration 4989, train loss: 0.197, average train loss: 0.224\n",
      "10:39:20 INFO:iteration 4999, train loss: 0.264, average train loss: 0.226\n",
      "10:39:20 INFO:iteration 4999, testing\n",
      "10:39:20 INFO:average test loss: 1.572\n",
      "10:39:20 INFO:overall average test loss: 1.580\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "data = StockData(batch_size)\n",
    "model = BaseNet(44, 1)\n",
    "learning_rate = 1e-3\n",
    "train_iter = 5000\n",
    "test_iter = 10\n",
    "test_interval = 100\n",
    "save_interval = 1001\n",
    "init_model_path = ''\n",
    "save_model_path = 'base_net.pth'\n",
    "tensorboard_path = 'tensorboard/base_net2'\n",
    "interface = Interface(data, model, learning_rate, train_iter, test_iter, test_interval, save_interval, \n",
    "                      init_model_path, save_model_path, tensorboard_path)\n",
    "interface.baseline()\n",
    "interface.train()\n",
    "interface.test_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:25:28 INFO:baseline loss: 2.963\n",
      "06:25:28 INFO:prediction loss: 2.986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training stocks: 34617, number of testing stocks: 13843\n",
      "(tensor(1.00000e-02 *\n",
      "       [[-3.2372, -1.3788]], device='cuda:0'), tensor([[ 2.6590,  3.2660]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "data = StockData()\n",
    "model = BaseNet(44, 1)\n",
    "learning_rate = 1e-3\n",
    "train_iter = 5000\n",
    "test_iter = 10\n",
    "test_interval = 100\n",
    "save_interval = 1001\n",
    "init_model_path = 'base_net.pth'\n",
    "save_model_path = ''\n",
    "tensorboard_path = ''\n",
    "interface = Interface(data, model, learning_rate, train_iter, test_iter, test_interval, save_interval, \n",
    "                      init_model_path, save_model_path, tensorboard_path)\n",
    "x, pred, y = interface.predict()\n",
    "print(pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
